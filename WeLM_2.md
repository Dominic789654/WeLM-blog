# WeLM-258B MoE：后训练 (Post-Training) 技术与实战总结

[Link](https://welm.weixin.qq.com/posts/welm-v3-post/)

## 核心导读

本文基于微信 AI 团队训练 WeLM-V3-258B-A22B MoE 模型的经验，详细披露了大模型在**后训练阶段（监督微调 SFT + 强化学习 RL）**的完整流水线。如果说预训练是赋予模型“通识底蕴”，那么后训练就是通过严苛的训练与规则，将其调教成一个逻辑严密、不胡说八道的“顶尖高手”。

---

## 1. 冷启动阶段 (SFT)：拒绝“死记硬背”，拓宽探索边界

目前业界的共识是：SFT 阶段如果过度训练，会压缩模型在后续 RL 阶段的策略探索空间。因此，WeLM 的冷启动目标是**构建具备强探索潜力的模型**，核心策略是“多样性”：

* **问题与数据多样化**：通过严格的质量校验、标签分类、语义聚类和分桶处理，筛选出分布均衡的高质量样本。同时采用 MinHash-LSH 和语义相似度过滤，严格规避数据泄露污染。
* **思维链 (CoT) 瘦身与发散**：引入 ConCISE 技术，专门裁剪思维链中因“不确定性”产生的冗余验证步骤。
* **💡 核心启发（“八股文”比喻）**：这一步的目的是防止模型形成“计算-验证-反复确认-输出”的固化、僵硬范式。通过精简与发散，模型被塑造成一个拥有“非线性复杂思维网络”的灵活解题者，而不是一个只会背题的“书呆子”。

---

## 2. 强化学习 (RL)：两阶段 GRPO 与混合验证体系

RL 训练以 GRPO 算法为主，分为“先攻数学、后攻混合任务”的两阶段流程。

### 2.1 第一阶段：死磕数学任务

* **题型改造防作弊**：将选择题和判断题统一改写为问答题，从根本上杜绝模型在 RL 阶段靠“随机猜对”骗取奖励 (Reward Hacking)。
* **混合验证器 (Hybrid Verifier)**：面对 LaTeX、Markdown 等复杂的数学答案格式，WeLM 拒绝将其简化为单一数字，而是采用“规则 + 模型”的分层验证策略（规则判错的，再让模型二次召回）。

**表 1：不同数学验证模式在 HardVerify-Math 测试集的准确率对比**

| 验证方法 | 规则验证 | 模型验证 | **混合验证** |
| --- | --- | --- | --- |
| Accuracy | 76.8% | 78.4% | **94.4%** |

*数据表明，混合验证极大地提升了验证准确率，同时保住了训练数据的真实难度。*

### 2.2 第二阶段：混合任务与 GRM (生成式奖励模型)

* **任务混合与平衡**：融合了推理型（数学、编程）与通用型（STEM、创作、文档问答）任务，并通过调控数据比例防止模型“偏科”。
* **引入 GRM 验证器**：针对没有绝对标准答案的通用任务，引入生成式奖励模型 (GenRM / WeChat-GRM)。它对推理过程和结果同步打分，极大地增强了 RL 训练的稳定性。

**表 2：不同 Verifier（奖励模型）在高难度测试集下的表现**

| Model | Accuracy |
| --- | --- |
| DeepSeek-V3-0324 | 57.9% |
| QwQ-32B | 58.3% |
| Deepseek-R1-0528 | 73.6% |
| **WeChat-GRM-32B** | **78.5%** |

*可以看出，WeChat-GRM 作为一个“铁面判官”，在高难度评测上的准确率显著优于其他顶级开源模型。*

---

## 3. 训练稳定性策略：防崩溃的“底层黑科技”

在大规模 MoE 模型上跑 RL 极易崩溃。WeLM 团队分享了 5 个填平工程深坑的关键策略：

* **提高专家路由精度 (FP64 Routing)**：在 MoE 路由层采用 52 位小数精度的 FP64，花极小的计算代价，避免精度漂移导致的专家误选。
* **异常序列掩码 (Anomaly Sequence Masking)**：融合文本特征与 KL 散度，把底层推理引擎（如 vLLM）和训练框架算子差异导致的“幽灵轨迹”自适应掩码掉，不让它们污染梯度。
* **分布偏差校正 (IcePop)**：通过双向截断与动态掩码机制，强行拉平长距离训练中“训练引擎”与“推理引擎”之间不断累积的概率分布偏差。
* **熵控制策略 (Clip-Cov)**：限制高协方差 Token 的梯度更新，强迫模型跳出“过度自信”的舒适区，保留持续探索更优解的欲望。
* **基于重复检测的提前截断**：一旦发现连续 N 个 Token 的预测概率高于 0.99（进入“复读机”状态），立即停止生成。这既稳住了训练，又节省了算力消耗。

---

## 4. 核心评测表现 (Evaluation Results)

在统一限制最大推理长度为 32K Tokens 的条件下，WeLM-258B 展现出了强大的竞争力。以下截取部分核心基准测试结果：

**表 3：Thinking (思考) 模型核心评测对比 (32K 上下文限制)**

| 基准测试 (类别) | **WeLM-258B** (Thinking-32K) | DeepSeek-R1 (0528-32K) | Qwen3-235B (Thinking-32K) |
| --- | --- | --- | --- |
| **MMLU-Pro** (知识) | **84.1** | 83.8 | 83.3 |
| **GPQA-Diamond** (困难知识) | 79.3 | 78.9 | **80.6** |
| **AIME25** (数学) | 82.2 | **83.3** | 77.3 |
| **ZebraLogic** (逻辑推理) | 96.3 | 94.4 | **96.8** |
| **SocialBench** (角色扮演) | **82.4** | **82.4** | 80.4 |

**表 4：Instruct (指令) 模型核心评测对比 (32K 上下文限制)**

| 基准测试 (类别) | **WeLM-258B** (Instruct) | DeepSeek-V3.2 (Instruct) | Qwen3-235B (Instruct) |
| --- | --- | --- | --- |
| **MMLU-Pro** (知识) | **84.1** | 84.0 | 79.0 |
| **AIME25** (数学) | **78.3** | 56.1 | 68.6 |
| **DROP** (推理) | 86.0 | 86.0 | **87.2** |
| **SocialBench** (角色扮演) | **86.1** | 84.9 | 84.0 |
| **Tau-2 retail** (工具调用) | 71.9 | **77.2** | 74.6 |

*注：上述表格中加粗部分代表在该列对比模型中的最佳成绩。*

**总结**：WeLM 的后训练实践证明，高质量的 SFT 决定了模型探索的广度，设计精良的 RL（GRPO + 混合验证器）决定了模型推理的深度，而极其硬核的底层维稳机制（FP64、异常掩码、概率截断）则是保证这艘巨轮能够顺利抵达彼岸的压舱石。在数学、推理和角色扮演等任务上，WeLM 均交出了非常能打的答卷。

